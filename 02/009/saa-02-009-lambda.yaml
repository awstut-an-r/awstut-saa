AWSTemplateFormatVersion: 2010-09-09

Parameters:
  Architecture:
    Type: String

  DAXClusterArn:
    Type: String
    
  DAXClusterDiscoveryEndpointURL:
    Type: String
    
  DynamoDBTable:
    Type: String
  
  EphemeralStorageSize:
    Type: Number
    
  FunctionSecurityGroup:
    Type: String
    
  FunctionSubnet:
    Type: String

  Handler:
    Type: String
    
  LayerPackage:
    Type: String
    
  LayerS3Bucket:
    Type: String
    
  LayerS3Key:
    Type: String
    
  Prefix:
    Type: String

  Runtime:
    Type: String
    
  Timeout:
    Type: Number
    
    
Resources:
  RequirementsParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Ref Prefix
      Type: String
      Value: |
        amazon-dax-client
        
  LambdaLayer:
    Type: AWS::Lambda::LayerVersion
    DependsOn:
      - CustomResource
    Properties:
      CompatibleArchitectures:
        - !Ref Architecture
      CompatibleRuntimes:
        - !Ref Runtime
      Content:
        S3Bucket: !Ref LayerS3Bucket
        S3Key: !Ref LayerS3Key
      Description: !Ref Prefix
      LayerName: !Ref Prefix
      
  CustomResource:
    Type: Custom::CustomResource
    Properties:
      ServiceToken: !GetAtt Function1.Arn

  Function1:
    Type: AWS::Lambda::Function
    Properties:
      Architectures:
        - !Ref Architecture
      Environment:
        Variables:
          LAYER_PACKAGE: !Ref LayerPackage
          REGION: !Ref AWS::Region
          REQUIREMENTS_PARAMETER: !Ref RequirementsParameter
          S3_BUCKET: !Ref LayerS3Bucket
          S3_BUCKET_FOLDER: !Ref Prefix
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import os
          import pip
          import shutil
          import subprocess
          
          layer_package = os.environ['LAYER_PACKAGE']
          region = os.environ['REGION']
          requirements_parameter = os.environ['REQUIREMENTS_PARAMETER']
          s3_bucket = os.environ['S3_BUCKET']
          s3_bucket_folder = os.environ['S3_BUCKET_FOLDER']
          
          CREATE = 'Create'
          #CREATE = 'Update'
          response_data = {}
          
          work_dir = '/tmp'
          requirements_file = 'requirements.txt'
          package_dir = 'python'
          
          requirements_path = os.path.join(work_dir, requirements_file)
          package_dir_path = os.path.join(work_dir, package_dir)
          layer_package_path = os.path.join(
            work_dir,
            layer_package
            )
          
          def lambda_handler(event, context):
            try:
              if event['RequestType'] == CREATE:
                ssm_client = boto3.client('ssm', region_name=region)
                ssm_response = ssm_client.get_parameter(Name=requirements_parameter)
                requirements = ssm_response['Parameter']['Value']
                
                with open(requirements_path, 'w') as file_data:
                  print(requirements, file=file_data)
                
                pip.main(['install', '-t', package_dir_path, '-r', requirements_path])
                shutil.make_archive(
                  os.path.splitext(layer_package_path)[0],
                  format='zip',
                  root_dir=work_dir,
                  base_dir=package_dir
                  )
                
                s3_resource = boto3.resource('s3')
                bucket = s3_resource.Bucket(s3_bucket)
                
                bucket.upload_file(
                  layer_package_path,
                  '/'.join([s3_bucket_folder, layer_package])
                  )
                
              cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
                
            except Exception as e:
              print(e)
              cfnresponse.send(event, context, cfnresponse.FAILED, response_data)
      EphemeralStorage:
        Size: !Ref EphemeralStorageSize
      FunctionName: !Sub "${Prefix}-function1"
      Handler: !Ref Handler
      Runtime: !Ref Runtime
      Role: !GetAtt FunctionRole1.Arn
      Timeout: !Ref Timeout
      
  FunctionRole1:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action: sts:AssumeRole
            Principal:
              Service:
                - lambda.amazonaws.com
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: CreateLambdaLayerPackagePolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - ssm:GetParameter
                Resource:
                  - !Sub "arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${RequirementsParameter}"
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource:
                  - !Sub "arn:aws:s3:::${LayerS3Bucket}/*"
      
      
  Function2:
    Type: AWS::Lambda::Function
    Properties:
      Architectures:
        - !Ref Architecture
      Code:
        ZipFile: |
          import amazondax
          import json
          import os
          
          dax_endpoint_url = os.environ['DAX_ENDPOINT_URL']
          dynamodb_table = os.environ['DYNAMODB_TABLE']
          region = os.environ['REGION']
          
          def lambda_handler(event, context):
            dax_client = amazondax.AmazonDaxClient(
              endpoint_url=dax_endpoint_url,
              region_name=region
              )
              
            partion_value = '1'
          
            result = dax_client.query(
              TableName=dynamodb_table,
              ExpressionAttributeNames={
                '#name0': 'partition_key'
              },
              ExpressionAttributeValues={
                ':value0': {'N': partion_value}
              },
              KeyConditionExpression='#name0 = :value0'
            )
            print(result)
              
            return {
              'statusCode': 200,
              'body': json.dumps(result, indent=2)
            }
      Environment:
        Variables:
          DAX_ENDPOINT_URL: !Ref DAXClusterDiscoveryEndpointURL
          DYNAMODB_TABLE: !Ref DynamoDBTable
          REGION: !Ref AWS::Region
      FunctionName: !Sub "${Prefix}-function2"
      Handler: !Ref Handler
      Layers:
        - !Ref LambdaLayer
      Runtime: !Ref Runtime
      Role: !GetAtt FunctionRole2.Arn
      VpcConfig:
        SecurityGroupIds:
          - !Ref FunctionSecurityGroup
        SubnetIds:
          - !Ref FunctionSubnet

  FunctionRole2:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action: sts:AssumeRole
            Principal:
              Service:
                - lambda.amazonaws.com
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole
      Policies:
        - PolicyName: DAXFullAccessPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "dax:*"
                Resource:
                  - !Ref DAXClusterArn
